# normalization-techniques

- Methods for disentangled-representation learning
  1. Batch normalization: [Batch Normalization, Google, ICML 2015]
  2. Layer normalization: [Layer Normalization, University of Toronto, 2016]
  3. Instance normalization: [Instance Normalization: The Missing Ingredient for Fast Stylization, 2016]
  4. Group normalization: [Group Normalization, Facebook AI Research, ECCV 2018]
  5. Weight Standardization: [Weight Standardization, Johns Hopkins University, 2019] 
  
- Dependencies
  - Python 3.6+

### Reference
1. [Batch Normalization, Google, ICML 2015]
2. [Layer Normalization, University of Toronto, 2016]
3. [Instance Normalization: The Missing Ingredient for Fast Stylization, 2016]
4. [Group Normalization, Facebook AI Research, ECCV 2018]
5. [Weight Standardization, Johns Hopkins University, 2019]

[Batch Normalization, Google, ICML 2015]: http://proceedings.mlr.press/v37/ioffe15.pdf
[Layer Normalization, University of Toronto, 2016]: https://arxiv.org/pdf/1607.06450.pdf
[Instance Normalization: The Missing Ingredient for Fast Stylization, 2016]: https://arxiv.org/pdf/1607.08022.pdf
[Group Normalization, Facebook AI Research, ECCV 2018]: https://eccv2018.org/openaccess/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf
[Weight Standardization, Johns Hopkins University, 2019]: https://arxiv.org/abs/1903.10520
